{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "804eed7b",
   "metadata": {},
   "source": [
    "## Prediction Generation Pipeline\n",
    "\n",
    "This Notebook defines the pipline for generatin prediction from the saved models. It is run by papermill package in Apache Airflow and predictions are generated accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01581d0",
   "metadata": {},
   "source": [
    "##### Set the working directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bef2548-a555-4a0e-a9a2-25486d3251aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters injected by Papermill or default fallback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    parameters\n",
    "except NameError:\n",
    "    parameters = {\"cwd\": \".\", \"airflow\": True}  # Add \"airflow\": True default to avoid preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2099802-54f5-4400-8ec8-a1d988a2ce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(f\"ðŸ“¥ Working directory set to: {cwd}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f23daa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory\n",
    "import os\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Save models - FIXED XGBoost saving\n",
    "for name, model in models.items():\n",
    "    if name == 'XGBoost':\n",
    "        # Ensure we have an XGBoost model object\n",
    "        if hasattr(model, 'save_model'):\n",
    "            model.save_model(f\"models/{name}_model.json\")  # Preferred XGBoost format\n",
    "        else:\n",
    "            # Fallback if it's not a proper XGBoost model\n",
    "            import pickle\n",
    "            with open(f\"models/{name}_model.pkl\", 'wb') as f:\n",
    "                pickle.dump(model, f)\n",
    "    else:\n",
    "        # Keras models\n",
    "        model.save(f\"models/{name}_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb491e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: Dense\n",
      "Loaded model: GRU\n",
      "Loaded model: LSTM\n",
      "Loaded model: transformer\n",
      "Failed to load XGBoost model from ./models/XGBoost_model.json: File format not supported: filepath=./models/XGBoost_model.json. Keras 3 only supports V3 `.keras` files and legacy H5 format files (`.h5` extension). Note that the legacy SavedModel format is not supported by `load_model()` in Keras 3. In order to reload a TensorFlow SavedModel as an inference-only layer in Keras 3, use `keras.layers.TFSMLayer(./models/XGBoost_model.json, call_endpoint='serving_default')` (note that your `call_endpoint` might have a different name).\n",
      "\n",
      "Dense Predictions (ddd_demand):\n",
      "April 2025: 1.616\n",
      "May 2025: 1.582\n",
      "June 2025: 1.504\n",
      "\n",
      "GRU Predictions (ddd_demand):\n",
      "April 2025: 1.866\n",
      "May 2025: 1.671\n",
      "June 2025: 1.245\n",
      "\n",
      "LSTM Predictions (ddd_demand):\n",
      "April 2025: 1.532\n",
      "May 2025: 1.498\n",
      "June 2025: 1.413\n",
      "\n",
      "transformer Predictions (ddd_demand):\n",
      "April 2025: 1.483\n",
      "May 2025: 1.610\n",
      "June 2025: 1.391\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "\n",
    "# === Step 1: Load your selected_data dataframe ===\n",
    "# Assumes 'selected_data' already available with required columns\n",
    "# ['avg_temp_max', 'avg_temp_min', 'avg_humidity', 'total_precipitation', 'total_sunshine_hours', 'ddd_demand']\n",
    "\n",
    "# === Step 2: Extract last 12 months as input (Apr 2024 â€“ Mar 2025) ===\n",
    "selected_data = pd.read_csv(\"selected_data.csv\")\n",
    "input_window = selected_data.iloc[-12:].copy()\n",
    "input_raw = input_window.values.astype('float32')\n",
    "\n",
    "num_samples = len(data_values)\n",
    "num_train = int(0.60 * num_samples)\n",
    "num_val = int(0.20 * num_samples)\n",
    "num_test = num_samples - num_train - num_val\n",
    "\n",
    "# Compute stats ONLY from training data\n",
    "mean = data_values[:num_train].mean(axis=0)  # Mean per feature\n",
    "std = data_values[:num_train].std(axis=0)    # Std per feature\n",
    "std[std < 1e-10] = 1.0  # Avoid division by zero\n",
    "\n",
    "# === Step 3: Apply normalization (use training mean/std) ===\n",
    "input_normalized = (input_raw - mean) / std\n",
    "\n",
    "# === Step 4: Drop 'ddd_demand' to get input features only ===\n",
    "input_features = input_normalized[:, :-1]  # shape: (12, 5)\n",
    "input_keras = input_features.reshape(1, 12, 5)  # shape: (1, 12, 5)\n",
    "\n",
    "# === Step 5: Load models ===\n",
    "model_paths = {\n",
    "    'Dense': './models/Dense_model.keras',\n",
    "    'GRU': './models/GRU_model.keras',\n",
    "    'LSTM': './models/LSTM_model.keras',\n",
    "    'transformer':'./models/transformer_model.keras',\n",
    "    'XGBoost':'./models/XGBoost_model.json'\n",
    "}\n",
    "\n",
    "models = {}\n",
    "for name, path in model_paths.items():\n",
    "    try:\n",
    "        models[name] = keras.models.load_model(path)\n",
    "        print(f\"Loaded model: {name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {name} model from {path}: {e}\")\n",
    "\n",
    "# === Step 6: Predict and Denormalize ===\n",
    "ddd_mean = mean[-1]\n",
    "ddd_std = std[-1]\n",
    "\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        y_pred = model.predict(input_keras, verbose=0).flatten()  # shape (3,)\n",
    "        y_pred_orig = y_pred * ddd_std + ddd_mean\n",
    "\n",
    "        print(f\"\\n{name} Predictions (ddd_demand):\")\n",
    "        for i, month in enumerate([\"April 2025\", \"May 2025\", \"June 2025\"]):\n",
    "            print(f\"{month}: {y_pred_orig[i]:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction with {name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6d70784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Dense loaded\n",
      "âœ“ GRU loaded\n",
      "âœ“ LSTM loaded\n",
      "âœ“ transformer loaded\n",
      "\n",
      "Dense:\n",
      "  Apr 2025: 1.62\n",
      "  May 2025: 1.58\n",
      "  Jun 2025: 1.50\n",
      "\n",
      "GRU:\n",
      "  Apr 2025: 1.87\n",
      "  May 2025: 1.67\n",
      "  Jun 2025: 1.24\n",
      "\n",
      "LSTM:\n",
      "  Apr 2025: 1.53\n",
      "  May 2025: 1.50\n",
      "  Jun 2025: 1.41\n",
      "\n",
      "transformer:\n",
      "  Apr 2025: 1.48\n",
      "  May 2025: 1.61\n",
      "  Jun 2025: 1.39\n",
      "\n",
      "âœ… Saved 12 predictions\n",
      "Dates: 2025-04-01, 2025-05-01, 2025-06-01\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# === Step 1: Load and verify dataset ===\n",
    "selected_data = pd.read_csv(\"selected_data.csv\")\n",
    "\n",
    "# Verify we have header + 99 datapoints (Jan 2017-Mar 2025)\n",
    "assert len(selected_data) == 99, f\"Expected 99 datapoints (100 lines with header), got {len(selected_data)}\"\n",
    "\n",
    "# === Generate date sequence programmatically ===\n",
    "start_date = pd.Timestamp('2017-01-01')\n",
    "end_date = start_date + relativedelta(months=98)  # 0-indexed (Jan 2017 = month 0)\n",
    "assert end_date == pd.Timestamp('2025-03-1'), \"Date math failed - last point should be Mar 2025\"\n",
    "\n",
    "# Generate prediction dates (next 3 months after last datapoint)\n",
    "prediction_dates = [end_date + relativedelta(months=i) for i in range(1, 4)]\n",
    "date_strings = [date.strftime('%Y-%m-%d') for date in prediction_dates]\n",
    "\n",
    "# === Prepare input data ===\n",
    "input_raw = selected_data.iloc[-12:].values.astype('float32')  # Last 12 months\n",
    "\n",
    "# === Normalization ===\n",
    "mean = data_values[:num_train].mean(axis=0)  # Mean per feature\n",
    "std = data_values[:num_train].std(axis=0)    # Std per feature\n",
    "std[std < 1e-10] = 1.0\n",
    "\n",
    "input_normalized = (input_raw - mean) / std\n",
    "input_features = input_normalized[:, :-1]  # Exclude target\n",
    "input_keras = input_features.reshape(1, 12, 5)\n",
    "\n",
    "# === Load models ===\n",
    "model_paths = {\n",
    "    'Dense': './models/Dense_model.keras',\n",
    "    'GRU': './models/GRU_model.keras',\n",
    "    'LSTM': './models/LSTM_model.keras',\n",
    "    'transformer': './models/transformer_model.keras'\n",
    "}\n",
    "\n",
    "models = {}\n",
    "for name, path in model_paths.items():\n",
    "    try:\n",
    "        models[name] = keras.models.load_model(path)\n",
    "        print(f\"âœ“ {name} loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— {name} failed: {str(e)}\")\n",
    "\n",
    "# === Make predictions ===\n",
    "ddd_mean = mean[-1]\n",
    "ddd_std = std[-1]\n",
    "\n",
    "predictions = []\n",
    "for model_name, model in models.items():\n",
    "    try:\n",
    "        preds = model.predict(input_keras, verbose=0).flatten()\n",
    "        denorm_preds = preds * ddd_std + ddd_mean\n",
    "        \n",
    "        for date_str, pred in zip(date_strings, denorm_preds):\n",
    "            predictions.append({\n",
    "                'model_name': model_name,\n",
    "                'date': date_str,\n",
    "                'predicted_demand': float(pred)  # Ensure native Python float\n",
    "            })\n",
    "        \n",
    "        print(f\"\\n{model_name}:\")\n",
    "        for date, pred in zip(prediction_dates, denorm_preds):\n",
    "            print(f\"  {date.strftime('%b %Y')}: {pred:.2f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâš  {model_name} failed: {str(e)}\")\n",
    "\n",
    "# === Save results ===\n",
    "if predictions:\n",
    "    pd.DataFrame(predictions).to_csv(r\"data/predictions.csv\", \n",
    "                                   index=False,\n",
    "                                   float_format='%.4f',\n",
    "                                   columns=['model_name', 'date', 'predicted_demand'])\n",
    "    print(f\"\\nâœ… Saved {len(predictions)} predictions\")\n",
    "    print(\"Dates:\", \", \".join(date_strings))\n",
    "else:\n",
    "    print(\"\\nâŒ No predictions generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cf170e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Load training data (adjust path and range as needed)\n",
    "data_path = 'data/selected_data.csv'\n",
    "df = pd.read_csv(data_path, names=[\n",
    "    'avg_temp_max', 'avg_temp_min', 'avg_humidity',\n",
    "    'total_precipitation', 'total_sunshine_hours', 'ddd_demand'\n",
    "])\n",
    "\n",
    "# Optionally, use a subset (e.g., exclude test period)\n",
    "df_train = df.iloc[:-12]  # Exclude last 12 months for training\n",
    "\n",
    "# Calculate mean and std\n",
    "mean = df_train.mean().values\n",
    "std = df_train.std().values\n",
    "\n",
    "# Save to JSON\n",
    "params = {'mean': mean.tolist(), 'std': std.tolist()}\n",
    "with open('models/normalization_params.json', 'w') as f:\n",
    "    json.dump(params, f, indent=4)\n",
    "print(\"Saved normalization_params.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
