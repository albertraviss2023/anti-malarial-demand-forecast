{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "804eed7b",
   "metadata": {},
   "source": [
    "## Prediction Generation Pipeline\n",
    "\n",
    "This Notebook defines the pipline for generatin prediction from the saved models. It is run by papermill package in Apache Airflow and predictions are generated accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db76d68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 06:15:08.926792: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-24 06:15:08.931626: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-24 06:15:09.092467: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-24 06:15:09.689038: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-24 06:15:11.649366: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# ====================\n",
    "# 0. ENV SETUP\n",
    "# ====================\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01581d0",
   "metadata": {},
   "source": [
    "##### Set the working directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27f77b04-ed5c-45c4-bb2d-ff2f45eb93a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:üìÅ Working directory set to: /home/notebooks\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Auto-detect parameters injected by Papermill (Airflow) vs local\n",
    "try:\n",
    "    parameters\n",
    "except NameError:\n",
    "    # We're in Jupyter, set cwd to the notebook folder\n",
    "    current_file_path = Path(__file__).parent if \"__file__\" in globals() else Path().resolve()\n",
    "    parameters = {\"cwd\": str(current_file_path)}\n",
    "\n",
    "# Use provided or default cwd\n",
    "cwd = Path(parameters.get(\"cwd\", \".\")).resolve()\n",
    "logger.info(f\"üìÅ Working directory set to: {cwd}\")\n",
    "\n",
    "# Example use\n",
    "data_path = cwd / \"data\" / \"somefile.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bbbf32-f3c3-4fb2-b8a3-6779e8db9e64",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2099802-54f5-4400-8ec8-a1d988a2ce57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:‚ùå Data not found at: ./data/raw_climate_ddd_merged_data.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "raw_climate_ddd_merged_data.csv not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(data_path):\n\u001b[1;32m      7\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚ùå Data not found at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_climate_ddd_merged_data.csv not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m raw_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(data_path)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Reorder columns if needed (ensure 'ddd_demand' is last)\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: raw_climate_ddd_merged_data.csv not found"
     ]
    }
   ],
   "source": [
    "# ====================\n",
    "# 1. LOAD INPUT DATA\n",
    "# ====================\n",
    "# Adjust path to account for notebook running in docker/notebooks/\n",
    "data_path = os.path.join(cwd, \"data\", \"raw_climate_ddd_merged_data.csv\")\n",
    "if not os.path.exists(data_path):\n",
    "    logger.error(f\"‚ùå Data not found at: {data_path}\")\n",
    "    raise FileNotFoundError(\"raw_climate_ddd_merged_data.csv not found\")\n",
    "\n",
    "raw_data = pd.read_csv(data_path)\n",
    "\n",
    "# Reorder columns if needed (ensure 'ddd_demand' is last)\n",
    "feature_cols = [\n",
    "    'avg_temp_max', 'avg_temp_min', 'avg_humidity',\n",
    "    'total_precipitation', 'total_sunshine_hours', 'ddd_demand'\n",
    "]\n",
    "selected_data = raw_data[feature_cols]\n",
    "selected_data = selected_data.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9ec4481-e94e-4327-a8e0-f7b3cc31dc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column order in selected_data:\n",
      "Index(['avg_temp_max', 'avg_temp_min', 'avg_humidity', 'total_precipitation',\n",
      "       'total_sunshine_hours', 'ddd_demand'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Column order in selected_data:\")\n",
    "print(selected_data.columns)  # Should match feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5773b5b0-91f5-4baa-b89f-0db3aded70a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:‚úÖ Loaded normalization parameters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Std values (after clipping): [1.62568307e+00 5.85483909e-01 8.69638824e+00 6.30080176e+03\n",
      " 1.92188159e+03 2.59629726e-01]\n",
      "Mean values (after clipping): [2.74350853e+01 1.78718662e+01 7.20621948e+01 1.27884971e+04\n",
      " 3.68978633e+04 1.39756978e+00]\n"
     ]
    }
   ],
   "source": [
    "# ====================\n",
    "# 2. LOAD NORMALIZATION PARAMS\n",
    "# ====================\n",
    "norm_path = os.path.join(cwd,\"models\", \"normalization_params.json\")\n",
    "if not os.path.exists(norm_path):\n",
    "    logger.error(f\"‚ùå Normalization file not found at: {norm_path}\")\n",
    "    raise FileNotFoundError(\"normalization_params.json not found\")\n",
    "\n",
    "with open(norm_path, \"r\") as f:\n",
    "    norm_params = json.load(f)\n",
    "\n",
    "mean = np.array(norm_params[\"mean\"])\n",
    "std = np.array(norm_params[\"std\"])\n",
    "std[std < 1e-10] = 1.0  # Avoid divide by zero\n",
    "\n",
    "logger.info(\"‚úÖ Loaded normalization parameters.\")\n",
    "print(\"Std values (after clipping):\", std)\n",
    "print(\"Mean values (after clipping):\", mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8534e5d-23e9-4a12-8b8d-a99aff811d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================\n",
    "# 3. EXTRACT INPUT WINDOW (Last 12 months)\n",
    "# ====================\n",
    "input_window = selected_data.iloc[-12:].copy()\n",
    "input_raw = input_window.values  # shape: (12, 6)\n",
    "\n",
    "# Normalize using precomputed training mean/std\n",
    "input_normalized = (input_raw - mean) / std\n",
    "\n",
    "# Keep only input features (exclude 'ddd_demand')\n",
    "input_features = input_normalized[:, :-1]  # shape: (12, 5)\n",
    "input_keras = input_features.reshape(1, 12, 5)  # shape: (1, 12, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cd516e6-9d4d-4719-96e6-6d193d98b8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:üìÜ Predicting for: April 2025, May 2025, June 2025\n"
     ]
    }
   ],
   "source": [
    "# ====================\n",
    "# 4. DETERMINE TARGET MONTHS\n",
    "# ====================\n",
    "# Get year and month from last record in full dataset\n",
    "last_year = int(raw_data[\"year\"].iloc[-1])\n",
    "last_month = int(raw_data[\"month\"].iloc[-1])\n",
    "\n",
    "# Generate month names for next 3 months\n",
    "prediction_months = []\n",
    "for i in range(1, 4):\n",
    "    next_month = last_month + i\n",
    "    pred_year = last_year + (next_month - 1) // 12\n",
    "    pred_month = ((next_month - 1) % 12) + 1\n",
    "    prediction_months.append(datetime(pred_year, pred_month, 1).strftime(\"%B %Y\"))\n",
    "\n",
    "logger.info(f\"üìÜ Predicting for: {', '.join(prediction_months)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c26bb1-07a4-4b36-803d-a8099fd11800",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:‚úÖ Loaded model: Dense\n",
      "INFO:__main__:‚úÖ Loaded model: GRU\n",
      "INFO:__main__:‚úÖ Loaded model: LSTM\n",
      "INFO:__main__:‚úÖ Loaded model: transformer\n"
     ]
    }
   ],
   "source": [
    "# ====================\n",
    "# 5. LOAD MODELS\n",
    "# ====================\n",
    "model_paths = {\n",
    "    'Dense': os.path.join(cwd, \"models\", \"Dense_model.keras\"),\n",
    "    'GRU': os.path.join(cwd, \"models\", \"GRU_model.keras\"),\n",
    "    'LSTM': os.path.join(cwd, \"models\", \"LSTM_model.keras\"),\n",
    "    'transformer': os.path.join(cwd, \"models\", \"transformer_model.keras\"),\n",
    "}\n",
    "\n",
    "models = {}\n",
    "for name, path in model_paths.items():\n",
    "    try:\n",
    "        models[name] = keras.models.load_model(path)\n",
    "        logger.info(f\"‚úÖ Loaded model: {name}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"‚ö†Ô∏è Could not load model {name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26627cc3-6019-4893-966d-b7929983af92",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055fce0c-28ec-417a-b63c-e5137aac076e",
   "metadata": {},
   "source": [
    "### Cell 3: Make Predictions and Demornalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3d13d75-26f7-4210-aba0-ffbed206fe2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "üìà Dense Predictions (ddd_demand):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April 2025: 1.616\n",
      "May 2025: 1.582\n",
      "June 2025: 1.504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "üìà GRU Predictions (ddd_demand):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April 2025: 1.866\n",
      "May 2025: 1.671\n",
      "June 2025: 1.245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "üìà LSTM Predictions (ddd_demand):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April 2025: 1.532\n",
      "May 2025: 1.498\n",
      "June 2025: 1.413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "üìà transformer Predictions (ddd_demand):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April 2025: 1.483\n",
      "May 2025: 1.610\n",
      "June 2025: 1.391\n"
     ]
    }
   ],
   "source": [
    "# ====================\n",
    "# 6. MAKE PREDICTIONS\n",
    "# ====================\n",
    "ddd_mean = mean[-1]\n",
    "ddd_std = std[-1]\n",
    "\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        y_pred = model.predict(input_keras, verbose=0).flatten()\n",
    "        y_pred_orig = y_pred * ddd_std + ddd_mean\n",
    "\n",
    "        logger.info(f\"\\nüìà {name} Predictions (ddd_demand):\")\n",
    "        for i, month in enumerate(prediction_months):\n",
    "            print(f\"{month}: {y_pred_orig[i]:.3f}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error predicting with {name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e35537-7f20-4ba1-b816-4c97c18fcea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6078390d",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aca99b0-c77f-47c0-a39e-af5c51c1b0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:üíæ Saved predictions to .\\../..\\data\\predicted_demand_2025_03.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>date</th>\n",
       "      <th>predicted_demand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dense</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>1.6158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dense</td>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>1.5825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dense</td>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>1.5041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GRU</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>1.8661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GRU</td>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>1.6706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GRU</td>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>1.2447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>1.5321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>1.4979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>1.4128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>transformer</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>1.4832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>transformer</td>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>1.6103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>transformer</td>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>1.3905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     model_name        date  predicted_demand\n",
       "0         Dense  2025-04-01            1.6158\n",
       "1         Dense  2025-05-01            1.5825\n",
       "2         Dense  2025-06-01            1.5041\n",
       "3           GRU  2025-04-01            1.8661\n",
       "4           GRU  2025-05-01            1.6706\n",
       "5           GRU  2025-06-01            1.2447\n",
       "6          LSTM  2025-04-01            1.5321\n",
       "7          LSTM  2025-05-01            1.4979\n",
       "8          LSTM  2025-06-01            1.4128\n",
       "9   transformer  2025-04-01            1.4832\n",
       "10  transformer  2025-05-01            1.6103\n",
       "11  transformer  2025-06-01            1.3905"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================\n",
    "# 8. SAVE PREDICTIONS\n",
    "# ====================\n",
    "from datetime import date\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define output directory using cwd\n",
    "output_dir = os.path.join(cwd, \"data\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "logger.info(f\"Output directory created: {output_dir}\")\n",
    "\n",
    "# Determine base month string from last available data\n",
    "month_str = f\"{last_year}_{last_month:02d}\"\n",
    "output_path = os.path.join(output_dir, f\"predictions_{month_str}.csv\")\n",
    "logger.info(f\"Output path: {output_path}\")\n",
    "\n",
    "# Collect predictions\n",
    "prediction_records = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        y_pred = model.predict(input_keras, verbose=0).flatten()\n",
    "        y_pred_orig = y_pred * ddd_std + ddd_mean\n",
    "\n",
    "        for i, y in enumerate(y_pred_orig):\n",
    "            pred_year = last_year + ((last_month + i) // 12)\n",
    "            pred_month = ((last_month + i) % 12) + 1\n",
    "            pred_date = date(pred_year, pred_month, 1).isoformat()\n",
    "\n",
    "            prediction_records.append({\n",
    "                \"model_name\": name,\n",
    "                \"date\": pred_date,\n",
    "                \"predicted_demand\": round(float(y), 4)\n",
    "            })\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error predicting with {name}: {e}\")\n",
    "\n",
    "logger.info(f\"Number of prediction records: {len(prediction_records)}\")\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "pred_df = pd.DataFrame(prediction_records)\n",
    "pred_df.to_csv(output_path, index=False)\n",
    "logger.info(f\"üíæ Saved predictions to {output_path}\")\n",
    "\n",
    "# Verify file exists\n",
    "if os.path.exists(output_path):\n",
    "    logger.info(f\"File confirmed at {output_path}\")\n",
    "else:\n",
    "    logger.error(f\"File not found at {output_path}\")\n",
    "\n",
    "# Preview if running interactively\n",
    "if not parameters.get(\"airflow\", False):\n",
    "    display(pred_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
