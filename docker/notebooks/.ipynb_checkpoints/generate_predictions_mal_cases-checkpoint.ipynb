{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "804eed7b",
   "metadata": {},
   "source": [
    "## Prediction Generation Pipeline\n",
    "\n",
    "This Notebook defines the pipline for generatin prediction from the saved models. It is run by papermill package in Apache Airflow and predictions are generated accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db76d68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================\n",
    "# 0. ENV SETUP\n",
    "# ====================\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01581d0",
   "metadata": {},
   "source": [
    "##### Set the working directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27f77b04-ed5c-45c4-bb2d-ff2f45eb93a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Notebook detected as running in a non-Airflow (likely Jupyter) environment.\n",
      "INFO:__main__:Current working directory in Jupyter: /home/notebooks\n",
      "INFO:__main__:Changed current working directory to: /opt/data\n",
      "INFO:__main__:Final effective data directory for operations: /opt/data\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Determine the execution environment ---\n",
    "\n",
    "running_in_airflow = False\n",
    "# A common way to detect Airflow is by checking for Airflow-specific environment variables\n",
    "# For example, 'AIRFLOW_HOME' or 'AIRFLOW_CTX_DAG_ID' are good indicators.\n",
    "if os.environ.get('AIRFLOW_HOME') or os.environ.get('AIRFLOW_CTX_DAG_ID'):\n",
    "    running_in_airflow = True\n",
    "    logger.info(\"Notebook detected as running in Airflow.\")\n",
    "else:\n",
    "    logger.info(\"Notebook detected as running in a non-Airflow (likely Jupyter) environment.\")\n",
    "\n",
    "# --- Set the target data directory ---\n",
    "\n",
    "target_data_dir = Path(\"/opt/data\")\n",
    "\n",
    "# If running in Jupyter from /home/notebooks, we need to adjust the path\n",
    "if not running_in_airflow:\n",
    "    # Assuming the notebook is at /home/notebooks/your_notebook.ipynb\n",
    "    # and you want to access /opt/data\n",
    "    # In a typical Jupyter setup, os.getcwd() will be /home/notebooks\n",
    "    current_notebook_dir = Path(os.getcwd())\n",
    "    logger.info(f\"Current working directory in Jupyter: {current_notebook_dir}\")\n",
    "\n",
    "    # You might need to adjust this logic depending on how you mount /opt/data\n",
    "    # If /opt/data is directly accessible at the root level, then target_data_dir is fine as is.\n",
    "    # If /opt/data is mounted *relative* to your Jupyter environment in a non-standard way,\n",
    "    # you'd need to adjust target_data_dir to reflect that.\n",
    "    # For instance, if /opt/data is mounted as /home/notebooks/data_mount, you'd do:\n",
    "    # target_data_dir = current_notebook_dir / \"data_mount\"\n",
    "\n",
    "    # Given your requirement that /opt/data is the target, regardless of where the notebook runs,\n",
    "    # and assuming /opt/data is directly accessible from the root,\n",
    "    # the target_data_dir remains /opt/data.\n",
    "    pass # No change needed for target_data_dir if /opt/data is a root-level path\n",
    "\n",
    "# --- Ensure the target directory exists and set current working directory if needed ---\n",
    "\n",
    "if not target_data_dir.exists():\n",
    "    logger.warning(f\"Target data directory '{target_data_dir}' does not exist. Attempting to create it.\")\n",
    "    try:\n",
    "        target_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "    except OSError as e:\n",
    "        logger.error(f\"Failed to create directory {target_data_dir}: {e}\")\n",
    "        # Depending on your use case, you might want to raise an exception here\n",
    "        # or handle the error gracefully.\n",
    "\n",
    "# Change the current working directory if it's not already the target_data_dir\n",
    "# This is particularly useful for relative paths within your notebook's logic.\n",
    "if Path.cwd() != target_data_dir:\n",
    "    try:\n",
    "        os.chdir(target_data_dir)\n",
    "        logger.info(f\"Changed current working directory to: {os.getcwd()}\")\n",
    "    except OSError as e:\n",
    "        logger.error(f\"Failed to change working directory to {target_data_dir}: {e}\")\n",
    "\n",
    "logger.info(f\"Final effective data directory for operations: {Path.cwd()}\")\n",
    "\n",
    "# Now, any file operations in your notebook (e.g., Path(\"my_file.txt\").read_text())\n",
    "# will operate relative to /opt/data.\n",
    "\n",
    "# Example usage:\n",
    "# data_file_path = Path(\"example.txt\")\n",
    "# if not data_file_path.exists():\n",
    "#     data_file_path.write_text(\"Hello from the notebook!\")\n",
    "#     logger.info(f\"Created {data_file_path.name} in {Path.cwd()}\")\n",
    "# else:\n",
    "#     logger.info(f\"Content of {data_file_path.name}: {data_file_path.read_text()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bbbf32-f3c3-4fb2-b8a3-6779e8db9e64",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2099802-54f5-4400-8ec8-a1d988a2ce57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:✅ Successfully loaded malaria_historical.csv from /opt/data/malaria_historical.csv\n",
      "INFO:__main__:First 5 rows of data:\n",
      "   year  month  district  mal_cases  avg_temp_max  avg_temp_min  avg_humidity  \\\n",
      "0  2020      1      Abim     5945.0         30.00         18.60         51.77   \n",
      "1  2020      1  Adjumani    25321.0         33.31         20.08         42.10   \n",
      "2  2020      1     Agago    19090.0         32.09         19.14         48.42   \n",
      "3  2020      1  Alebtong     1450.0         32.11         19.72         47.16   \n",
      "4  2020      1  Amolatar     3373.0         29.64         20.06         64.35   \n",
      "\n",
      "   sum_precipitation  sum_sunshine_hours  \n",
      "0               55.8              320.49  \n",
      "1               13.7              327.06  \n",
      "2               42.8              322.06  \n",
      "3               43.9              305.70  \n",
      "4               68.7              314.29  \n"
     ]
    }
   ],
   "source": [
    "# ====================\n",
    "# 1. LOAD INPUT DATA\n",
    "# ====================\n",
    "\n",
    "# Since you've already changed the current working directory to /opt/data,\n",
    "# you can now simply refer to the file by its name.\n",
    "data_filename = \"malaria_historical.csv\"\n",
    "data_path = Path(data_filename) # Path object automatically resolves relative to cwd\n",
    "\n",
    "if not data_path.exists():\n",
    "    logger.error(f\"❌ Data not found at: {data_path.absolute()}\") # Use .absolute() for full path in error message\n",
    "    raise FileNotFoundError(f\"{data_filename} not found in the current working directory ({Path.cwd()})\")\n",
    "\n",
    "try:\n",
    "    raw_data = pd.read_csv(data_path)\n",
    "    logger.info(f\"✅ Successfully loaded {data_filename} from {data_path.absolute()}\")\n",
    "    logger.info(f\"First 5 rows of data:\\n{raw_data.head()}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading data from {data_path.absolute()}: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c87cffc-d7bb-4f08-bfd4-add4b9caf6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index(['year', 'month', 'district', 'mal_cases', 'avg_temp_max',\n",
      "       'avg_temp_min', 'avg_humidity', 'sum_precipitation',\n",
      "       'sum_sunshine_hours'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(type(raw_data))  # Should output: <class 'pandas.core.frame.DataFrame'>\n",
    "print(raw_data.columns)  # Check if all columns exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5aee9507-3b22-4d7c-881e-6b835ceb6443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples for Kamuli: 1\n",
      "      year  month district  mal_cases  avg_temp_max  avg_temp_min  \\\n",
      "7479  2025      4   Kamuli    11911.0         26.57         18.87   \n",
      "\n",
      "      avg_humidity  sum_precipitation  sum_sunshine_hours  \n",
      "7479         83.87              178.5              285.57  \n"
     ]
    }
   ],
   "source": [
    "# Filter rows for Kamuli district\n",
    "kamuli_data = raw_data[raw_data['district'] == 'Kamuli']\n",
    "\n",
    "# Display the number of rows and preview the data\n",
    "print(f\"Total samples for Kamuli: {len(kamuli_data)}\")\n",
    "print(kamuli_data.head(10))  # or kamuli_data.tail(), or just kamuli_data to see all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5773b5b0-91f5-4baa-b89f-0db3aded70a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ====================\n",
    "# 1. FEATURE SELECTION\n",
    "# ====================\n",
    "feature_cols = [\n",
    "    'district', 'year','month', # Add district column\n",
    "    'avg_temp_max', 'avg_temp_min', 'avg_humidity',\n",
    "    'sum_precipitation', 'sum_sunshine_hours', 'mal_cases'\n",
    "]\n",
    "\n",
    "# Extract selected columns\n",
    "selected_data = raw_data[feature_cols]\n",
    "\n",
    "# Convert to NumPy array (keeping district as string)\n",
    "data_values = selected_data.drop(columns=['district']).values.astype('float32')\n",
    "district_labels = selected_data['district'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b987107c-62fa-4048-a091-e326f1775cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Not enough training data for district Kamuli, skipping...\n",
      "📍 Abim → mean: [  28.68842    17.887894   62.01184    98.63947   318.86868  6338.5527  ], std: [2.3697593e+00 9.7348303e-01 1.5422560e+01 8.7819649e+01 1.4696355e+01\n",
      " 2.0782686e+03]\n",
      "📍 Adjumani → mean: [3.1614996e+01 2.0099737e+01 6.2325531e+01 7.1605270e+01 3.1614636e+02\n",
      " 2.6186447e+04], std: [2.4268689e+00 8.7700599e-01 1.6375797e+01 5.8883087e+01 1.4303403e+01\n",
      " 9.1575957e+03]\n",
      "📍 Agago → mean: [   30.909472    18.830788    60.056313    77.70264    316.6992\n",
      " 17938.395   ], std: [2.4519389e+00 9.0371865e-01 1.5902541e+01 7.1074310e+01 1.5716954e+01\n",
      " 8.2685703e+03]\n"
     ]
    }
   ],
   "source": [
    "# Initialize stats storage\n",
    "stats_per_district = {}\n",
    "\n",
    "# Normalize per district using training split\n",
    "for district, df_district in raw_data.groupby('district'):\n",
    "    df_district = df_district.sort_values(['year', 'month']).reset_index(drop=True)\n",
    "    \n",
    "    # Select only feature columns for normalization (excluding 'district', 'year', 'month')\n",
    "    selected_features = df_district[['avg_temp_max', 'avg_temp_min', 'avg_humidity',\n",
    "                                     'sum_precipitation', 'sum_sunshine_hours', 'mal_cases']]\n",
    "    data_values = selected_features.values.astype('float32')\n",
    "\n",
    "    # Split for normalization (first 60%)\n",
    "    num_samples = len(data_values)\n",
    "    num_train = int(0.60 * num_samples)\n",
    "\n",
    "    if num_train < 2:\n",
    "        print(f\"⚠️ Not enough training data for district {district}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    train_values = data_values[:num_train]\n",
    "\n",
    "    mean = train_values.mean(axis=0)\n",
    "    std = train_values.std(axis=0)\n",
    "    std[std < 1e-10] = 1.0  # avoid divide by zero\n",
    "\n",
    "    stats_per_district[district] = {'mean': mean, 'std': std}\n",
    "\n",
    "# Print preview\n",
    "for district in list(stats_per_district.keys())[:3]:\n",
    "    print(f\"📍 {district} → mean: {stats_per_district[district]['mean']}, std: {stats_per_district[district]['std']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a3d457-5381-4e88-adfe-f578fbcf3878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8c26bb1-07a4-4b36-803d-a8099fd11800",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:❌ Model directory not found at: /opt/malaria_models\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "The specified model directory /opt/malaria_models does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m models_base_dir\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     10\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m❌ Model directory not found at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodels_base_dir\u001b[38;5;241m.\u001b[39mabsolute()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe specified model directory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodels_base_dir\u001b[38;5;241m.\u001b[39mabsolute()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m models_base_dir\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[1;32m     13\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m❌ Path exists but is not a directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodels_base_dir\u001b[38;5;241m.\u001b[39mabsolute()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: The specified model directory /opt/malaria_models does not exist."
     ]
    }
   ],
   "source": [
    "# ====================\n",
    "# 5. LOAD MODELS\n",
    "# ====================\n",
    "\n",
    "# Define the absolute path to your models folder\n",
    "models_base_dir = Path(\"/opt/malaria_models/\")\n",
    "\n",
    "# Ensure the models base directory exists\n",
    "if not models_base_dir.exists():\n",
    "    logger.error(f\"❌ Model directory not found at: {models_base_dir.absolute()}\")\n",
    "    raise FileNotFoundError(f\"The specified model directory {models_base_dir.absolute()} does not exist.\")\n",
    "elif not models_base_dir.is_dir():\n",
    "    logger.error(f\"❌ Path exists but is not a directory: {models_base_dir.absolute()}\")\n",
    "    raise NotADirectoryError(f\"The specified model path {models_base_dir.absolute()} is not a directory.\")\n",
    "\n",
    "\n",
    "model_paths = {\n",
    "    'Dense': models_base_dir / \"dense.keras\",\n",
    "    'GRU': models_base_dir / \"GRU.keras\",\n",
    "    'LSTM': models_base_dir / \"LSTM.keras\",\n",
    "    'transformer': models_base_dir / \"transformer.keras\",\n",
    "}\n",
    "\n",
    "models = {}\n",
    "for name, path in model_paths.items():\n",
    "    if not path.exists():\n",
    "        logger.warning(f\"⚠️ Model file not found for '{name}' at: {path.absolute()}\")\n",
    "        continue # Skip to the next model if the file doesn't exist\n",
    "\n",
    "    try:\n",
    "        models[name] = keras.models.load_model(path)\n",
    "        logger.info(f\"✅ Loaded model: {name} from {path.absolute()}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"⚠️ Could not load model {name} from {path.absolute()}: {e}\")\n",
    "\n",
    "# You can now access your loaded models, e.g., models['Dense']\n",
    "if not models:\n",
    "    logger.error(\"❌ No models were loaded successfully.\")\n",
    "else:\n",
    "    logger.info(f\"Successfully loaded {len(models)} models: {', '.join(models.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26627cc3-6019-4893-966d-b7929983af92",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055fce0c-28ec-417a-b63c-e5137aac076e",
   "metadata": {},
   "source": [
    "### Cell 3: Make Predictions and Demornalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3d13d75-26f7-4210-aba0-ffbed206fe2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "📈 Dense Predictions (ddd_demand):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April 2025: 1.616\n",
      "May 2025: 1.582\n",
      "June 2025: 1.504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "📈 GRU Predictions (ddd_demand):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April 2025: 1.866\n",
      "May 2025: 1.671\n",
      "June 2025: 1.245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "📈 LSTM Predictions (ddd_demand):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April 2025: 1.532\n",
      "May 2025: 1.498\n",
      "June 2025: 1.413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "📈 transformer Predictions (ddd_demand):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April 2025: 1.483\n",
      "May 2025: 1.610\n",
      "June 2025: 1.391\n"
     ]
    }
   ],
   "source": [
    "# ====================\n",
    "# 6. MAKE PREDICTIONS\n",
    "# ====================\n",
    "ddd_mean = mean[-1]\n",
    "ddd_std = std[-1]\n",
    "\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        y_pred = model.predict(input_keras, verbose=0).flatten()\n",
    "        y_pred_orig = y_pred * ddd_std + ddd_mean\n",
    "\n",
    "        logger.info(f\"\\n📈 {name} Predictions (ddd_demand):\")\n",
    "        for i, month in enumerate(prediction_months):\n",
    "            print(f\"{month}: {y_pred_orig[i]:.3f}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Error predicting with {name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e35537-7f20-4ba1-b816-4c97c18fcea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6078390d",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aca99b0-c77f-47c0-a39e-af5c51c1b0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:💾 Saved predictions to .\\../..\\data\\predicted_demand_2025_03.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>date</th>\n",
       "      <th>predicted_demand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dense</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>1.6158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dense</td>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>1.5825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dense</td>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>1.5041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GRU</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>1.8661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GRU</td>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>1.6706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GRU</td>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>1.2447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>1.5321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>1.4979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>1.4128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>transformer</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>1.4832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>transformer</td>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>1.6103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>transformer</td>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>1.3905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     model_name        date  predicted_demand\n",
       "0         Dense  2025-04-01            1.6158\n",
       "1         Dense  2025-05-01            1.5825\n",
       "2         Dense  2025-06-01            1.5041\n",
       "3           GRU  2025-04-01            1.8661\n",
       "4           GRU  2025-05-01            1.6706\n",
       "5           GRU  2025-06-01            1.2447\n",
       "6          LSTM  2025-04-01            1.5321\n",
       "7          LSTM  2025-05-01            1.4979\n",
       "8          LSTM  2025-06-01            1.4128\n",
       "9   transformer  2025-04-01            1.4832\n",
       "10  transformer  2025-05-01            1.6103\n",
       "11  transformer  2025-06-01            1.3905"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================\n",
    "# 8. SAVE PREDICTIONS\n",
    "# ====================\n",
    "from datetime import date\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define output directory using cwd\n",
    "output_dir = os.path.join(cwd, \"data\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "logger.info(f\"Output directory created: {output_dir}\")\n",
    "\n",
    "# Determine base month string from last available data\n",
    "month_str = f\"{last_year}_{last_month:02d}\"\n",
    "output_path = os.path.join(output_dir, f\"predictions_{month_str}.csv\")\n",
    "logger.info(f\"Output path: {output_path}\")\n",
    "\n",
    "# Collect predictions\n",
    "prediction_records = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        y_pred = model.predict(input_keras, verbose=0).flatten()\n",
    "        y_pred_orig = y_pred * ddd_std + ddd_mean\n",
    "\n",
    "        for i, y in enumerate(y_pred_orig):\n",
    "            pred_year = last_year + ((last_month + i) // 12)\n",
    "            pred_month = ((last_month + i) % 12) + 1\n",
    "            pred_date = date(pred_year, pred_month, 1).isoformat()\n",
    "\n",
    "            prediction_records.append({\n",
    "                \"model_name\": name,\n",
    "                \"date\": pred_date,\n",
    "                \"predicted_demand\": round(float(y), 4)\n",
    "            })\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Error predicting with {name}: {e}\")\n",
    "\n",
    "logger.info(f\"Number of prediction records: {len(prediction_records)}\")\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "pred_df = pd.DataFrame(prediction_records)\n",
    "pred_df.to_csv(output_path, index=False)\n",
    "logger.info(f\"💾 Saved predictions to {output_path}\")\n",
    "\n",
    "# Verify file exists\n",
    "if os.path.exists(output_path):\n",
    "    logger.info(f\"File confirmed at {output_path}\")\n",
    "else:\n",
    "    logger.error(f\"File not found at {output_path}\")\n",
    "\n",
    "# Preview if running interactively\n",
    "if not parameters.get(\"airflow\", False):\n",
    "    display(pred_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
