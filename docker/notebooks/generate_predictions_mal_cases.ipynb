{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "804eed7b",
   "metadata": {},
   "source": [
    "## Prediction Generation Pipeline\n",
    "\n",
    "This Notebook defines the pipline for generatin prediction from the saved models. It is run by papermill package in Apache Airflow and predictions are generated accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db76d68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 06:15:08.926792: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-24 06:15:08.931626: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-24 06:15:09.092467: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-24 06:15:09.689038: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-24 06:15:11.649366: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# ====================\n",
    "# 0. ENV SETUP\n",
    "# ====================\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01581d0",
   "metadata": {},
   "source": [
    "##### Set the working directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27f77b04-ed5c-45c4-bb2d-ff2f45eb93a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:üìÅ Working directory set to: /home/notebooks\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Auto-detect parameters injected by Papermill (Airflow) vs local Jupyter\n",
    "if running_in_airflow:\n",
    "    cwd = Path(parameters.get(\"cwd\", \".\")).resolve()\n",
    "else:\n",
    "    # Jupyter or local interactive: force to /opt/data (not the current notebook folder)\n",
    "    cwd = Path(\"/opt/data\").resolve()\n",
    "\n",
    "# Set working directory\n",
    "if running_in_airflow:\n",
    "    cwd = Path(parameters.get(\"cwd\", \".\")).resolve()\n",
    "else:\n",
    "    # Jupyter or local interactive: force to /opt/data\n",
    "    cwd = Path(\"/opt/data\").resolve()\n",
    "\n",
    "logger.info(f\"üìÅ Working directory set to: {cwd}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bbbf32-f3c3-4fb2-b8a3-6779e8db9e64",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2099802-54f5-4400-8ec8-a1d988a2ce57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>district</th>\n",
       "      <th>mal_cases</th>\n",
       "      <th>avg_temp_max</th>\n",
       "      <th>avg_temp_min</th>\n",
       "      <th>avg_humidity</th>\n",
       "      <th>sum_precipitation</th>\n",
       "      <th>sum_sunshine_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>Abim</td>\n",
       "      <td>5945.0</td>\n",
       "      <td>30.00</td>\n",
       "      <td>18.60</td>\n",
       "      <td>51.77</td>\n",
       "      <td>55.8</td>\n",
       "      <td>320.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>Adjumani</td>\n",
       "      <td>25321.0</td>\n",
       "      <td>33.31</td>\n",
       "      <td>20.08</td>\n",
       "      <td>42.10</td>\n",
       "      <td>13.7</td>\n",
       "      <td>327.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>Agago</td>\n",
       "      <td>19090.0</td>\n",
       "      <td>32.09</td>\n",
       "      <td>19.14</td>\n",
       "      <td>48.42</td>\n",
       "      <td>42.8</td>\n",
       "      <td>322.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>Alebtong</td>\n",
       "      <td>1450.0</td>\n",
       "      <td>32.11</td>\n",
       "      <td>19.72</td>\n",
       "      <td>47.16</td>\n",
       "      <td>43.9</td>\n",
       "      <td>305.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>Amolatar</td>\n",
       "      <td>3373.0</td>\n",
       "      <td>29.64</td>\n",
       "      <td>20.06</td>\n",
       "      <td>64.35</td>\n",
       "      <td>68.7</td>\n",
       "      <td>314.29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  district  mal_cases  avg_temp_max  avg_temp_min  avg_humidity  \\\n",
       "0  2020      1      Abim     5945.0         30.00         18.60         51.77   \n",
       "1  2020      1  Adjumani    25321.0         33.31         20.08         42.10   \n",
       "2  2020      1     Agago    19090.0         32.09         19.14         48.42   \n",
       "3  2020      1  Alebtong     1450.0         32.11         19.72         47.16   \n",
       "4  2020      1  Amolatar     3373.0         29.64         20.06         64.35   \n",
       "\n",
       "   sum_precipitation  sum_sunshine_hours  \n",
       "0               55.8              320.49  \n",
       "1               13.7              327.06  \n",
       "2               42.8              322.06  \n",
       "3               43.9              305.70  \n",
       "4               68.7              314.29  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====================\n",
    "# 1. LOAD INPUT DATA\n",
    "# ====================\n",
    "# Adjust path to account for notebook running in docker/notebooks/\n",
    "data_path = os.path.join(cwd, \"malaria_historical.csv\")\n",
    "if not os.path.exists(data_path):\n",
    "    logger.error(f\"‚ùå Data not found at: {data_path}\")\n",
    "    raise FileNotFoundError(\"malaria_historical.csv not found\")\n",
    "\n",
    "raw_data = pd.read_csv(data_path)\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c87cffc-d7bb-4f08-bfd4-add4b9caf6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index(['year', 'month', 'district', 'mal_cases', 'avg_temp_max',\n",
      "       'avg_temp_min', 'avg_humidity', 'sum_precipitation',\n",
      "       'sum_sunshine_hours'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(type(raw_data))  # Should output: <class 'pandas.core.frame.DataFrame'>\n",
    "print(raw_data.columns)  # Check if all columns exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9ec4481-e94e-4327-a8e0-f7b3cc31dc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column order in selected_data:\n",
      "Index(['avg_temp_max', 'avg_temp_min', 'avg_humidity', 'total_precipitation',\n",
      "       'total_sunshine_hours', 'ddd_demand'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Column order in selected_data:\")\n",
    "print(selected_data.columns)  # Should match feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5aee9507-3b22-4d7c-881e-6b835ceb6443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples for Kamuli: 1\n",
      "      year  month district  mal_cases  avg_temp_max  avg_temp_min  \\\n",
      "7479  2025      4   Kamuli    11911.0         26.57         18.87   \n",
      "\n",
      "      avg_humidity  sum_precipitation  sum_sunshine_hours  \n",
      "7479         83.87              178.5              285.57  \n"
     ]
    }
   ],
   "source": [
    "# Filter rows for Kamuli district\n",
    "kamuli_data = raw_data[raw_data['district'] == 'Kamuli']\n",
    "\n",
    "# Display the number of rows and preview the data\n",
    "print(f\"Total samples for Kamuli: {len(kamuli_data)}\")\n",
    "print(kamuli_data.head(10))  # or kamuli_data.tail(), or just kamuli_data to see all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5773b5b0-91f5-4baa-b89f-0db3aded70a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ====================\n",
    "# 1. FEATURE SELECTION\n",
    "# ====================\n",
    "feature_cols = [\n",
    "    'district', 'year','month', # Add district column\n",
    "    'avg_temp_max', 'avg_temp_min', 'avg_humidity',\n",
    "    'sum_precipitation', 'sum_sunshine_hours', 'mal_cases'\n",
    "]\n",
    "\n",
    "# Extract selected columns\n",
    "selected_data = raw_data[feature_cols]\n",
    "\n",
    "# Convert to NumPy array (keeping district as string)\n",
    "data_values = selected_data.drop(columns=['district']).values.astype('float32')\n",
    "district_labels = selected_data['district'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b987107c-62fa-4048-a091-e326f1775cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Not enough training data for district Kamuli, skipping...\n",
      "üìç Abim ‚Üí mean: [  28.68842    17.887894   62.01184    98.63947   318.86868  6338.5527  ], std: [2.3697593e+00 9.7348303e-01 1.5422560e+01 8.7819649e+01 1.4696355e+01\n",
      " 2.0782686e+03]\n",
      "üìç Adjumani ‚Üí mean: [3.1614996e+01 2.0099737e+01 6.2325531e+01 7.1605270e+01 3.1614636e+02\n",
      " 2.6186447e+04], std: [2.4268689e+00 8.7700599e-01 1.6375797e+01 5.8883087e+01 1.4303403e+01\n",
      " 9.1575957e+03]\n",
      "üìç Agago ‚Üí mean: [   30.909472    18.830788    60.056313    77.70264    316.6992\n",
      " 17938.395   ], std: [2.4519389e+00 9.0371865e-01 1.5902541e+01 7.1074310e+01 1.5716954e+01\n",
      " 8.2685703e+03]\n"
     ]
    }
   ],
   "source": [
    "# Initialize stats storage\n",
    "stats_per_district = {}\n",
    "\n",
    "# Normalize per district using training split\n",
    "for district, df_district in raw_data.groupby('district'):\n",
    "    df_district = df_district.sort_values(['year', 'month']).reset_index(drop=True)\n",
    "    \n",
    "    # Select only feature columns for normalization (excluding 'district', 'year', 'month')\n",
    "    selected_features = df_district[['avg_temp_max', 'avg_temp_min', 'avg_humidity',\n",
    "                                     'sum_precipitation', 'sum_sunshine_hours', 'mal_cases']]\n",
    "    data_values = selected_features.values.astype('float32')\n",
    "\n",
    "    # Split for normalization (first 60%)\n",
    "    num_samples = len(data_values)\n",
    "    num_train = int(0.60 * num_samples)\n",
    "\n",
    "    if num_train < 2:\n",
    "        print(f\"‚ö†Ô∏è Not enough training data for district {district}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    train_values = data_values[:num_train]\n",
    "\n",
    "    mean = train_values.mean(axis=0)\n",
    "    std = train_values.std(axis=0)\n",
    "    std[std < 1e-10] = 1.0  # avoid divide by zero\n",
    "\n",
    "    stats_per_district[district] = {'mean': mean, 'std': std}\n",
    "\n",
    "# Print preview\n",
    "for district in list(stats_per_district.keys())[:3]:\n",
    "    print(f\"üìç {district} ‚Üí mean: {stats_per_district[district]['mean']}, std: {stats_per_district[district]['std']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8534e5d-23e9-4a12-8b8d-a99aff811d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================\n",
    "# 3. EXTRACT INPUT WINDOW (Last 12 months)\n",
    "# ====================\n",
    "input_window = selected_data.iloc[-12:].copy()\n",
    "input_raw = input_window.values  # shape: (12, 6)\n",
    "\n",
    "# Normalize using precomputed training mean/std\n",
    "input_normalized = (input_raw - mean) / std\n",
    "\n",
    "# Keep only input features (exclude 'ddd_demand')\n",
    "input_features = input_normalized[:, :-1]  # shape: (12, 5)\n",
    "input_keras = input_features.reshape(1, 12, 5)  # shape: (1, 12, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8c26bb1-07a4-4b36-803d-a8099fd11800",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:‚ö†Ô∏è Could not load model Dense: File not found: filepath=/home/notebooks/models/Dense_model.keras. Please ensure the file is an accessible `.keras` zip file.\n",
      "WARNING:__main__:‚ö†Ô∏è Could not load model GRU: File not found: filepath=/home/notebooks/models/GRU_model.keras. Please ensure the file is an accessible `.keras` zip file.\n",
      "WARNING:__main__:‚ö†Ô∏è Could not load model LSTM: File not found: filepath=/home/notebooks/models/LSTM_model.keras. Please ensure the file is an accessible `.keras` zip file.\n",
      "WARNING:__main__:‚ö†Ô∏è Could not load model transformer: File not found: filepath=/home/notebooks/models/transformer_model.keras. Please ensure the file is an accessible `.keras` zip file.\n"
     ]
    }
   ],
   "source": [
    "# ====================\n",
    "# 5. LOAD MODELS\n",
    "# ====================\n",
    "cwd = Path(parameters.get(\"cwd\", \".\")).resolve()\n",
    "model_paths = {\n",
    "    'Dense': os.path.join(cwd, \"models\", \"Dense_model.keras\"),\n",
    "    'GRU': os.path.join(cwd, \"models\", \"GRU_model.keras\"),\n",
    "    'LSTM': os.path.join(cwd, \"models\", \"LSTM_model.keras\"),\n",
    "    'transformer': os.path.join(cwd, \"models\", \"transformer_model.keras\"),\n",
    "}\n",
    "\n",
    "models = {}\n",
    "for name, path in model_paths.items():\n",
    "    try:\n",
    "        models[name] = keras.models.load_model(path)\n",
    "        logger.info(f\"‚úÖ Loaded model: {name}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"‚ö†Ô∏è Could not load model {name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26627cc3-6019-4893-966d-b7929983af92",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055fce0c-28ec-417a-b63c-e5137aac076e",
   "metadata": {},
   "source": [
    "### Cell 3: Make Predictions and Demornalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3d13d75-26f7-4210-aba0-ffbed206fe2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "üìà Dense Predictions (ddd_demand):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April 2025: 1.616\n",
      "May 2025: 1.582\n",
      "June 2025: 1.504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "üìà GRU Predictions (ddd_demand):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April 2025: 1.866\n",
      "May 2025: 1.671\n",
      "June 2025: 1.245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "üìà LSTM Predictions (ddd_demand):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April 2025: 1.532\n",
      "May 2025: 1.498\n",
      "June 2025: 1.413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "üìà transformer Predictions (ddd_demand):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April 2025: 1.483\n",
      "May 2025: 1.610\n",
      "June 2025: 1.391\n"
     ]
    }
   ],
   "source": [
    "# ====================\n",
    "# 6. MAKE PREDICTIONS\n",
    "# ====================\n",
    "ddd_mean = mean[-1]\n",
    "ddd_std = std[-1]\n",
    "\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        y_pred = model.predict(input_keras, verbose=0).flatten()\n",
    "        y_pred_orig = y_pred * ddd_std + ddd_mean\n",
    "\n",
    "        logger.info(f\"\\nüìà {name} Predictions (ddd_demand):\")\n",
    "        for i, month in enumerate(prediction_months):\n",
    "            print(f\"{month}: {y_pred_orig[i]:.3f}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error predicting with {name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e35537-7f20-4ba1-b816-4c97c18fcea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6078390d",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aca99b0-c77f-47c0-a39e-af5c51c1b0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:üíæ Saved predictions to .\\../..\\data\\predicted_demand_2025_03.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>date</th>\n",
       "      <th>predicted_demand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dense</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>1.6158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dense</td>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>1.5825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dense</td>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>1.5041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GRU</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>1.8661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GRU</td>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>1.6706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GRU</td>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>1.2447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>1.5321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>1.4979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>1.4128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>transformer</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>1.4832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>transformer</td>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>1.6103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>transformer</td>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>1.3905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     model_name        date  predicted_demand\n",
       "0         Dense  2025-04-01            1.6158\n",
       "1         Dense  2025-05-01            1.5825\n",
       "2         Dense  2025-06-01            1.5041\n",
       "3           GRU  2025-04-01            1.8661\n",
       "4           GRU  2025-05-01            1.6706\n",
       "5           GRU  2025-06-01            1.2447\n",
       "6          LSTM  2025-04-01            1.5321\n",
       "7          LSTM  2025-05-01            1.4979\n",
       "8          LSTM  2025-06-01            1.4128\n",
       "9   transformer  2025-04-01            1.4832\n",
       "10  transformer  2025-05-01            1.6103\n",
       "11  transformer  2025-06-01            1.3905"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================\n",
    "# 8. SAVE PREDICTIONS\n",
    "# ====================\n",
    "from datetime import date\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define output directory using cwd\n",
    "output_dir = os.path.join(cwd, \"data\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "logger.info(f\"Output directory created: {output_dir}\")\n",
    "\n",
    "# Determine base month string from last available data\n",
    "month_str = f\"{last_year}_{last_month:02d}\"\n",
    "output_path = os.path.join(output_dir, f\"predictions_{month_str}.csv\")\n",
    "logger.info(f\"Output path: {output_path}\")\n",
    "\n",
    "# Collect predictions\n",
    "prediction_records = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        y_pred = model.predict(input_keras, verbose=0).flatten()\n",
    "        y_pred_orig = y_pred * ddd_std + ddd_mean\n",
    "\n",
    "        for i, y in enumerate(y_pred_orig):\n",
    "            pred_year = last_year + ((last_month + i) // 12)\n",
    "            pred_month = ((last_month + i) % 12) + 1\n",
    "            pred_date = date(pred_year, pred_month, 1).isoformat()\n",
    "\n",
    "            prediction_records.append({\n",
    "                \"model_name\": name,\n",
    "                \"date\": pred_date,\n",
    "                \"predicted_demand\": round(float(y), 4)\n",
    "            })\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error predicting with {name}: {e}\")\n",
    "\n",
    "logger.info(f\"Number of prediction records: {len(prediction_records)}\")\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "pred_df = pd.DataFrame(prediction_records)\n",
    "pred_df.to_csv(output_path, index=False)\n",
    "logger.info(f\"üíæ Saved predictions to {output_path}\")\n",
    "\n",
    "# Verify file exists\n",
    "if os.path.exists(output_path):\n",
    "    logger.info(f\"File confirmed at {output_path}\")\n",
    "else:\n",
    "    logger.error(f\"File not found at {output_path}\")\n",
    "\n",
    "# Preview if running interactively\n",
    "if not parameters.get(\"airflow\", False):\n",
    "    display(pred_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
