{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "804eed7b",
   "metadata": {},
   "source": [
    "## Prediction Generation Pipeline\n",
    "\n",
    "This Notebook defines the pipline for generatin prediction from the saved models. It is run by papermill package in Apache Airflow and predictions are generated accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db76d68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================\n",
    "# 0. ENV SETUP\n",
    "# ====================\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01581d0",
   "metadata": {},
   "source": [
    "##### Set the working directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27f77b04-ed5c-45c4-bb2d-ff2f45eb93a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory set to: C:\\Users\\alber\\anti-malarial-demand-forecast\\data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set data directory to ../data relative to current notebook\n",
    "data_dir = Path(\"../..\", \"data\").resolve()\n",
    "\n",
    "# Change working directory\n",
    "os.chdir(data_dir)\n",
    "\n",
    "print(f\"Current working directory set to: {os.getcwd()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bbbf32-f3c3-4fb2-b8a3-6779e8db9e64",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2099802-54f5-4400-8ec8-a1d988a2ce57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:‚úÖ Successfully loaded malaria_historical.csv from C:\\Users\\alber\\anti-malarial-demand-forecast\\data\\malaria_historical.csv\n",
      "INFO:__main__:First 5 rows of data:\n",
      "   year  month  district  mal_cases  avg_temp_max  avg_temp_min  avg_humidity  \\\n",
      "0  2020      1      Abim     5945.0         30.00         18.60         51.77   \n",
      "1  2020      1  Adjumani    25321.0         33.31         20.08         42.10   \n",
      "2  2020      1     Agago    19090.0         32.09         19.14         48.42   \n",
      "3  2020      1  Alebtong     1450.0         32.11         19.72         47.16   \n",
      "4  2020      1  Amolatar     3373.0         29.64         20.06         64.35   \n",
      "\n",
      "   sum_precipitation  sum_sunshine_hours  \n",
      "0               55.8              320.49  \n",
      "1               13.7              327.06  \n",
      "2               42.8              322.06  \n",
      "3               43.9              305.70  \n",
      "4               68.7              314.29  \n"
     ]
    }
   ],
   "source": [
    "# ====================\n",
    "# 1. LOAD INPUT DATA\n",
    "# ====================\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Since you've already changed the current working directory to /opt/data,\n",
    "# you can now simply refer to the file by its name.\n",
    "data_filename = \"malaria_historical.csv\"\n",
    "data_path = Path(data_filename) # Path object automatically resolves relative to cwd\n",
    "\n",
    "if not data_path.exists():\n",
    "    logger.error(f\"‚ùå Data not found at: {data_path.absolute()}\") # Use .absolute() for full path in error message\n",
    "    raise FileNotFoundError(f\"{data_filename} not found in the current working directory ({Path.cwd()})\")\n",
    "\n",
    "try:\n",
    "    raw_data = pd.read_csv(data_path)\n",
    "    logger.info(f\"‚úÖ Successfully loaded {data_filename} from {data_path.absolute()}\")\n",
    "    logger.info(f\"First 5 rows of data:\\n{raw_data.head()}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading data from {data_path.absolute()}: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c87cffc-d7bb-4f08-bfd4-add4b9caf6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index(['year', 'month', 'district', 'mal_cases', 'avg_temp_max',\n",
      "       'avg_temp_min', 'avg_humidity', 'sum_precipitation',\n",
      "       'sum_sunshine_hours'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(type(raw_data))  # Should output: <class 'pandas.core.frame.DataFrame'>\n",
    "print(raw_data.columns)  # Check if all columns exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8c26bb1-07a4-4b36-803d-a8099fd11800",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:üìÅ Current working directory: C:\\Users\\alber\\anti-malarial-demand-forecast\\data\n",
      "INFO:__main__:üì¶ Looking for models at: C:\\Users\\alber\\anti-malarial-demand-forecast\\malaria_models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Correct model directory relative to this notebook's cwd\n",
    "cwd = Path.cwd()\n",
    "model_dir = cwd.parent / \"malaria_models\"\n",
    "\n",
    "model_paths = {\n",
    "    'Dense': model_dir / \"dense.keras\",\n",
    "    'LSTM': model_dir / \"LSTM.keras\",\n",
    "    'GRU': model_dir / \"GRU.keras\",\n",
    "    'Transformer': model_dir / \"transformer.keras\",\n",
    "    'XGBoost': model_dir / \"XGBoost.json\",  # if you also load XGB separately\n",
    "}\n",
    "\n",
    "logger.info(f\"üìÅ Current working directory: {cwd}\")\n",
    "logger.info(f\"üì¶ Looking for models at: {model_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e53b6c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer\n",
    "from keras.saving import register_keras_serializable\n",
    "import tensorflow as tf\n",
    "\n",
    "@register_keras_serializable()\n",
    "class CastAndClipLayer(Layer):\n",
    "    def __init__(self, num_districts=118, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_districts = num_districts\n",
    "\n",
    "    def call(self, inputs):\n",
    "        clipped = tf.clip_by_value(inputs, 0, self.num_districts - 1)\n",
    "        return tf.cast(clipped, tf.int32)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"num_districts\": self.num_districts,\n",
    "        })\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30623a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\alber\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\alber\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "INFO:__main__:‚úÖ Loaded Keras model 'Dense' from C:\\Users\\alber\\anti-malarial-demand-forecast\\malaria_models\\dense.keras\n",
      "INFO:__main__:‚úÖ Loaded Keras model 'LSTM' from C:\\Users\\alber\\anti-malarial-demand-forecast\\malaria_models\\LSTM.keras\n",
      "INFO:__main__:‚úÖ Loaded Keras model 'GRU' from C:\\Users\\alber\\anti-malarial-demand-forecast\\malaria_models\\GRU.keras\n",
      "INFO:__main__:‚úÖ Loaded Keras model 'Transformer' from C:\\Users\\alber\\anti-malarial-demand-forecast\\malaria_models\\transformer.keras\n",
      "INFO:__main__:‚úÖ Loaded XGBoost model from: C:\\Users\\alber\\anti-malarial-demand-forecast\\malaria_models\\XGBoost.json\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import xgboost as xgb\n",
    "\n",
    "models = {}\n",
    "custom_objects = {'CastAndClipLayer': CastAndClipLayer}\n",
    "\n",
    "for name, path in model_paths.items():\n",
    "    if not path.exists():\n",
    "        logger.warning(f\"‚ö†Ô∏è Model file not found for '{name}' at: {path}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        if name == 'XGBoost':\n",
    "            booster = xgb.Booster()\n",
    "            booster.load_model(str(path))\n",
    "            models[name] = booster\n",
    "            logger.info(f\"‚úÖ Loaded XGBoost model from: {path}\")\n",
    "        else:\n",
    "            models[name] = load_model(path, custom_objects=custom_objects)\n",
    "            logger.info(f\"‚úÖ Loaded Keras model '{name}' from {path}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"‚ùå Failed to load model '{name}' from {path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6c664b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alber\\AppData\\Local\\Temp\\ipykernel_5048\\2155852125.py:26: RuntimeWarning: Mean of empty slice.\n",
      "  mean = data_values[:num_train].mean(axis=0)\n",
      "C:\\Users\\alber\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\numpy\\_core\\_methods.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "C:\\Users\\alber\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\numpy\\_core\\_methods.py:227: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "C:\\Users\\alber\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\numpy\\_core\\_methods.py:184: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "C:\\Users\\alber\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\numpy\\_core\\_methods.py:216: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n"
     ]
    }
   ],
   "source": [
    "# Normalization at district level\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "feature_cols = [\n",
    "    'avg_temp_max', 'avg_temp_min', 'avg_humidity',\n",
    "    'sum_precipitation', 'sum_sunshine_hours', 'mal_cases'\n",
    "]\n",
    "\n",
    "# Ensure required columns\n",
    "assert 'district' in raw_data.columns and 'year' in raw_data.columns and 'month' in raw_data.columns\n",
    "\n",
    "normalized_dfs = []\n",
    "stats_per_district = {}\n",
    "\n",
    "# Normalize per district\n",
    "for district, df_district in raw_data.groupby('district'):\n",
    "    df_district = df_district.sort_values(['year', 'month']).reset_index(drop=True)\n",
    "    selected_data = df_district[feature_cols]\n",
    "    data_values = selected_data.values.astype('float32')\n",
    "\n",
    "    # Split for normalization\n",
    "    num_samples = len(data_values)\n",
    "    num_train = int(0.60 * num_samples)\n",
    "\n",
    "    mean = data_values[:num_train].mean(axis=0)\n",
    "    std = data_values[:num_train].std(axis=0)\n",
    "    std[std < 1e-10] = 1.0  # avoid division by 0\n",
    "\n",
    "    stats_per_district[district] = {'mean': mean, 'std': std}\n",
    "    normalized_values = (data_values - mean) / std\n",
    "\n",
    "    normalized_df = pd.DataFrame(normalized_values, columns=feature_cols, index=selected_data.index)\n",
    "    normalized_df['district'] = district\n",
    "    normalized_df['year'] = df_district['year'].values\n",
    "    normalized_df['month'] = df_district['month'].values\n",
    "\n",
    "    normalized_dfs.append(normalized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b914787",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Combine and Sort Data Globally\n",
    "normalized_all_data = pd.concat(normalized_dfs).reset_index(drop=True)\n",
    "\n",
    "# Sort by district, year, month\n",
    "normalized_all_data = normalized_all_data.sort_values(['district', 'year', 'month']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f046d38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_temp_max</th>\n",
       "      <th>avg_temp_min</th>\n",
       "      <th>avg_humidity</th>\n",
       "      <th>sum_precipitation</th>\n",
       "      <th>sum_sunshine_hours</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>district_id</th>\n",
       "      <th>mal_cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.553466</td>\n",
       "      <td>0.731504</td>\n",
       "      <td>-0.664082</td>\n",
       "      <td>-0.487812</td>\n",
       "      <td>0.110320</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.189366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.734919</td>\n",
       "      <td>1.080765</td>\n",
       "      <td>-0.430009</td>\n",
       "      <td>-0.788428</td>\n",
       "      <td>0.087186</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.875033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.566126</td>\n",
       "      <td>1.337575</td>\n",
       "      <td>0.250164</td>\n",
       "      <td>0.258035</td>\n",
       "      <td>-0.149606</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.831246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.085064</td>\n",
       "      <td>0.978041</td>\n",
       "      <td>0.455058</td>\n",
       "      <td>0.748813</td>\n",
       "      <td>-2.350154</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.664761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.860180</td>\n",
       "      <td>-0.131377</td>\n",
       "      <td>1.202016</td>\n",
       "      <td>0.902537</td>\n",
       "      <td>0.811176</td>\n",
       "      <td>2020</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.576175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   avg_temp_max  avg_temp_min  avg_humidity  sum_precipitation  \\\n",
       "0      0.553466      0.731504     -0.664082          -0.487812   \n",
       "1      0.734919      1.080765     -0.430009          -0.788428   \n",
       "2      0.566126      1.337575      0.250164           0.258035   \n",
       "3      0.085064      0.978041      0.455058           0.748813   \n",
       "4     -0.860180     -0.131377      1.202016           0.902537   \n",
       "\n",
       "   sum_sunshine_hours  year  month  district_id  mal_cases  \n",
       "0            0.110320  2020      1            0  -0.189366  \n",
       "1            0.087186  2020      2            0  -0.875033  \n",
       "2           -0.149606  2020      3            0  -0.831246  \n",
       "3           -2.350154  2020      4            0  -0.664761  \n",
       "4            0.811176  2020      5            0   0.576175  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Step 3: Encode Districts and Rearrange Columns\n",
    "le = LabelEncoder()\n",
    "normalized_all_data['district_id'] = le.fit_transform(normalized_all_data['district'])\n",
    "\n",
    "# Move 'mal_cases' to last column and drop original 'district'\n",
    "cols = [c for c in normalized_all_data.columns if c not in ['mal_cases', 'district']] + ['mal_cases']\n",
    "normalized_all_data = normalized_all_data[cols]\n",
    "normalized_all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90e6734b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Original district names not found, using generated names\n",
      "Sample district names in mapping: ['District_0', 'District_1', 'District_2', 'District_3', 'District_4']\n",
      "Sample stats_per_district keys: ['Abim', 'Adjumani', 'Agago', 'Alebtong', 'Amolatar']\n"
     ]
    }
   ],
   "source": [
    "# 1. First, let's reconstruct the original LabelEncoder mapping\n",
    "# (Assuming you don't have the original 'le' object anymore)\n",
    "unique_district_ids = normalized_all_data['district_id'].unique()\n",
    "district_names = sorted(normalized_all_data['district'].unique()) if 'district' in normalized_all_data.columns else None\n",
    "\n",
    "# If you have the original district names column:\n",
    "if district_names is not None:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(district_names)\n",
    "    label_to_name = {i: name for i, name in enumerate(le.classes_)}\n",
    "else:\n",
    "    # If you only have IDs, create a dummy mapping\n",
    "    print(\"Warning: Original district names not found, using generated names\")\n",
    "    label_to_name = {i: f\"District_{i}\" for i in unique_district_ids}\n",
    "\n",
    "# 2. Verify stats_per_district keys match our label_to_name values\n",
    "print(\"Sample district names in mapping:\", list(label_to_name.values())[:5])\n",
    "print(\"Sample stats_per_district keys:\", list(stats_per_district.keys())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "685377ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of districts being processed: 118\n",
      "Found complete data for 118 out of 118 districts.\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001AAB816A160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001AAB816A160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001AAB816A160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001AAB816A160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Predictions successfully saved to predictions_may_june_july_2025.csv\n",
      "\n",
      "--- Sample Predictions for Abim ---\n",
      "  Model: Dense\n",
      "    - May 2025: 5259.26\n",
      "    - June 2025: 4791.98\n",
      "    - July 2025: 4210.12\n",
      "  Model: LSTM\n",
      "    - May 2025: 5422.97\n",
      "    - June 2025: 5053.59\n",
      "    - July 2025: 5015.16\n",
      "  Model: GRU\n",
      "    - May 2025: 5029.68\n",
      "    - June 2025: 4161.72\n",
      "    - July 2025: 4774.34\n",
      "  Model: Transformer\n",
      "    - May 2025: 5387.88\n",
      "    - June 2025: 5464.46\n",
      "    - July 2025: 5531.08\n",
      "  Model: XGBoost\n",
      "    - May 2025: 6293.69\n",
      "    - June 2025: 6281.08\n",
      "    - July 2025: 6207.02\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import xgboost as xgb  # <-- Import XGBoost\n",
    "from datetime import datetime\n",
    "\n",
    "# Step 1: Ensure Kamuli is removed and create district-to-ID mapping\n",
    "# This assumes 'raw_data' is already loaded and available.\n",
    "if 'Kamuli' in raw_data['district'].unique():\n",
    "    raw_data = raw_data[raw_data['district'] != 'Kamuli'].copy()\n",
    "unique_districts = sorted(raw_data['district'].unique())\n",
    "district_to_id = {district: idx for idx, district in enumerate(unique_districts)}\n",
    "num_districts = len(unique_districts)\n",
    "print(f\"Number of districts being processed: {num_districts}\")\n",
    "\n",
    "# Step 2: Prepare input data with district-level normalization\n",
    "def prepare_input_data(df, stats_per_district, start_date='2024-11', end_date='2025-04'):\n",
    "    \"\"\"\n",
    "    Prepare input data for prediction, normalizing features per district and skipping incomplete data.\n",
    "    \"\"\"\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month']].assign(day=1))\n",
    "    start = pd.to_datetime(start_date + '-01')\n",
    "    end = pd.to_datetime(end_date + '-01')\n",
    "    \n",
    "    input_df = df[(df['date'] >= start) & (df['date'] <= end)].copy()\n",
    "    \n",
    "    input_data = []\n",
    "    valid_districts = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    feature_cols = ['avg_temp_max', 'avg_temp_min', 'avg_humidity', 'sum_precipitation', 'sum_sunshine_hours']\n",
    "    \n",
    "    for district_idx, district in enumerate(unique_districts):\n",
    "        district_data = input_df[input_df['district'] == district].sort_values('date')\n",
    "        if len(district_data) != 6:\n",
    "            # print(f\"Warning: District {district} has {len(district_data)} months, expected 6. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        district_features = district_data[feature_cols].to_numpy()\n",
    "        \n",
    "        if district not in stats_per_district:\n",
    "            # print(f\"Warning: No normalization stats for {district}. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        mean = stats_per_district[district]['mean'][:-1]\n",
    "        std = stats_per_district[district]['std'][:-1]\n",
    "        std[std < 1e-10] = 1.0\n",
    "        normalized_features = (district_features - mean) / std\n",
    "        \n",
    "        district_id = np.full((6, 1), district_to_id[district])\n",
    "        district_input = np.concatenate([normalized_features, district_id], axis=-1)\n",
    "        \n",
    "        input_data.append(district_input)\n",
    "        valid_districts.append(district)\n",
    "        valid_indices.append(district_idx)\n",
    "    \n",
    "    if not input_data:\n",
    "        raise ValueError(\"No districts have complete data for the specified window.\")\n",
    "    \n",
    "    return np.array(input_data), valid_districts, valid_indices\n",
    "\n",
    "# Step 3: Prepare data for XGBoost\n",
    "def prepare_xgboost_data(batch_features, num_districts):\n",
    "    \"\"\"\n",
    "    Convert a batch of features to NumPy arrays for XGBoost.\n",
    "    \"\"\"\n",
    "    if isinstance(batch_features, tf.Tensor):\n",
    "        batch_features = batch_features.numpy()\n",
    "    \n",
    "    regular_features = batch_features[:, :, :-1]\n",
    "    district_ids = batch_features[:, :, -1]\n",
    "    district_ids_np = np.clip(district_ids[:, -1], 0, num_districts - 1).astype(np.int32)\n",
    "    \n",
    "    one_hot_encoder = tf.keras.layers.CategoryEncoding(num_tokens=num_districts, output_mode=\"one_hot\")\n",
    "    district_ids_encoded = one_hot_encoder(district_ids_np).numpy()\n",
    "    \n",
    "    regular_features_flat = regular_features.reshape(batch_features.shape[0], -1)\n",
    "    combined_features = np.concatenate([regular_features_flat, district_ids_encoded], axis=-1)\n",
    "    return combined_features, None\n",
    "\n",
    "# Step 4: Make predictions and denormalize\n",
    "def make_predictions(models_dict, input_data, valid_districts, stats_per_district):\n",
    "    \"\"\"\n",
    "    Generate and denormalize predictions for May, June, July 2025 for each district.\n",
    "    \"\"\"\n",
    "    predictions = {name: [] for name in models_dict}\n",
    "    \n",
    "    for model_name, model in models_dict.items():\n",
    "        if model_name != 'XGBoost':\n",
    "            y_pred = model.predict(input_data, verbose=0)\n",
    "        else:\n",
    "            # Prepare data specifically for XGBoost\n",
    "            X_flat, _ = prepare_xgboost_data(input_data, num_districts)\n",
    "            \n",
    "            # FIX: Convert NumPy array to DMatrix for XGBoost prediction\n",
    "            dmatrix_pred = xgb.DMatrix(X_flat)\n",
    "            y_pred = model.predict(dmatrix_pred)\n",
    "        \n",
    "        # Denormalize predictions\n",
    "        denorm_preds = np.zeros_like(y_pred)\n",
    "        for i, district in enumerate(valid_districts):\n",
    "            mean = stats_per_district[district]['mean'][-1]\n",
    "            std = stats_per_district[district]['std'][-1]\n",
    "            denorm_preds[i] = y_pred[i] * std + mean\n",
    "        \n",
    "        predictions[model_name] = denorm_preds\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Step 5: Export predictions to CSV\n",
    "def export_predictions(predictions, districts, filename='predictions_may_june_july_2025.csv'):\n",
    "    \"\"\"\n",
    "    Export predictions to a CSV file.\n",
    "    \"\"\"\n",
    "    output_data = []\n",
    "    months = ['May_2025', 'June_2025', 'July_2025']\n",
    "    \n",
    "    for district_idx, district in enumerate(districts):\n",
    "        for model_name in predictions:\n",
    "            preds = predictions[model_name][district_idx]\n",
    "            for month_idx, month in enumerate(months):\n",
    "                output_data.append({\n",
    "                    'district': district,\n",
    "                    'model': model_name,\n",
    "                    'month': month,\n",
    "                    'predicted_mal_cases': max(0, preds[month_idx]) # Ensure cases are non-negative\n",
    "                })\n",
    "\n",
    "    output_df = pd.DataFrame(output_data)\n",
    "    output_df.to_csv(filename, index=False)\n",
    "    print(f\"‚úÖ Predictions successfully saved to {filename}\")\n",
    "\n",
    "# --- Execution ---\n",
    "try:\n",
    "    # This assumes 'models', 'raw_data', and 'stats_per_district' are loaded in the environment\n",
    "    input_data, valid_districts, valid_indices = prepare_input_data(raw_data, stats_per_district, start_date='2024-11', end_date='2025-04')\n",
    "    print(f\"Found complete data for {len(valid_districts)} out of {num_districts} districts.\")\n",
    "    \n",
    "    predictions = make_predictions(models, input_data, valid_districts, stats_per_district)\n",
    "    export_predictions(predictions, valid_districts)\n",
    "    \n",
    "    # Optional: Print sample predictions for the first valid district\n",
    "    if valid_districts:\n",
    "        first_district = valid_districts[0]\n",
    "        print(f\"\\n--- Sample Predictions for {first_district} ---\")\n",
    "        for model_name in predictions:\n",
    "            preds = predictions[model_name][0]\n",
    "            print(f\"  Model: {model_name}\")\n",
    "            print(f\"    - May 2025: {preds[0]:.2f}\")\n",
    "            print(f\"    - June 2025: {preds[1]:.2f}\")\n",
    "            print(f\"    - July 2025: {preds[2]:.2f}\")\n",
    "            \n",
    "except (ValueError, NameError) as e:\n",
    "    print(f\"‚ùå An error occurred: {e}\")\n",
    "    print(\"Please ensure 'raw_data', 'stats_per_district', and 'models' are loaded correctly.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
