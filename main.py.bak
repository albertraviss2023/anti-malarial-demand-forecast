import os
import json
import logging
import numpy as np
import pandas as pd
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from tensorflow import keras
from datetime import datetime
from typing import Dict, Optional, List
from config import Config

# Setup logging
logging.basicConfig(
    filename='app.log',
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ModelManager:
    """Manages loading and serving ML models and data"""
    
    def __init__(self):
        self.models: Dict[str, keras.Model] = {}
        self.mean: Optional[np.ndarray] = None
        self.std: Optional[np.ndarray] = None
        self.test_maes: Optional[Dict[str, float]] = None
        self.data: Optional[pd.DataFrame] = None
        self.predictions: Optional[Dict[str, Dict[str, float]]] = None
        
    def initialize(self) -> None:
        """Load all required resources"""
        try:
            self._load_models()
            self._load_normalization_params()
            self._load_test_metrics()
            self._load_data()
            self._load_predictions()
            logger.info("ModelManager initialized successfully")
        except Exception as e:
            logger.error(f"Initialization failed: {str(e)}")
            raise HTTPException(status_code=500, detail="Initialization failed")
        
    def _load_models(self) -> None:
        """Load all Keras models from disk"""
        for name, filename in Config.MODEL_FILES.items():
            try:
                path = os.path.join(Config.MODELS_DIR, filename)
                if not os.path.exists(path):
                    raise FileNotFoundError(f"Model file {path} not found")
                self.models[name] = keras.models.load_model(path)
                logger.info(f"Loaded {name} model from {path}")
            except Exception as e:
                logger.error(f"Failed to load {name} model: {str(e)}")
                self.models[name] = None
    
    def _load_normalization_params(self) -> None:
        """Load mean and std for normalization"""
        try:
            params_path = os.path.join(Config.MODELS_DIR, 'normalization_params.json')
            if not os.path.exists(params_path):
                raise FileNotFoundError(f"Normalization params file {params_path} not found")
            with open(params_path) as f:
                params = json.load(f)
                self.mean = np.array(params['mean'], dtype='float32')
                self.std = np.array(params['std'], dtype='float32')
                if self.mean.shape != (6,) or self.std.shape != (6,):
                    raise ValueError("Invalid normalization params shape")
                if (self.std < 1e-10).any():
                    logger.warning("Near-zero std detected in normalization params, setting to 1.0")
                    self.std[self.std < 1e-10] = 1.0
                logger.info("Loaded normalization parameters")
        except Exception as e:
            logger.error(f"Failed to load normalization params: {str(e)}")
            self.mean = np.zeros(6, dtype='float32')
            self.std = np.ones(6, dtype='float32')
    
    def _load_test_metrics(self) -> None:
        """Load test metrics from CSV file"""
        try:
            metrics_path = os.path.join(Config.DATA_DIR, 'test_metrics.csv')
            if not os.path.exists(metrics_path):
                raise FileNotFoundError(f"Test metrics file {metrics_path} not found")
            metrics_df = pd.read_csv(metrics_path)
            required_cols = ['model_name', 'mae']
            if not all(col in metrics_df.columns for col in required_cols):
                raise ValueError(f"Missing required columns in test_metrics.csv: {required_cols}")
            if not all(model in metrics_df['model_name'].values for model in Config.MODEL_FILES.keys()):
                raise ValueError("Not all expected models found in test_metrics.csv")
            if (metrics_df['mae'] < 0).any():
                raise ValueError("Negative MAE values found in test_metrics.csv")
            self.test_maes = dict(zip(metrics_df['model_name'], metrics_df['mae']))
            logger.info("Loaded test metrics")
        except Exception as e:
            logger.error(f"Failed to load test metrics: {str(e)}")
            self.test_maes = {name: 0.0 for name in Config.MODEL_FILES.keys()}
    
    def _load_data(self) -> None:
        """Load and validate input data, selecting the last 12 months"""
        try:
            data_path = os.path.join(Config.DATA_DIR, 'selected_data.csv')
            if not os.path.exists(data_path):
                raise FileNotFoundError(f"Data file {data_path} not found")
            df = pd.read_csv(data_path, header=0)
            if not all(col in df.columns for col in Config.EXPECTED_COLS):
                raise ValueError(f"Expected columns {Config.EXPECTED_COLS}, got {df.columns.tolist()}")
            
            # Assign dates from Jan 2017 to Mar 2025 (99 months)
            start_date = pd.to_datetime('2017-01-01')
            num_months = len(df)
            if num_months != 99:
                raise ValueError(f"Expected exactly 99 months (Jan 2017 - Mar 2025), got {num_months}")
            dates = pd.date_range(start=start_date, periods=num_months, freq='MS')
            df.index = dates
            
            # Select last 12 months (Apr 2024 to Mar 2025)
            self.data = df.iloc[-12:].copy()
            if len(self.data) != 12:
                raise ValueError("Could not select exactly 12 months of data")
            
            # Validate data types and handle missing values
            self.data = self.data.astype({col: 'float32' for col in Config.EXPECTED_COLS})
            if self.data.isnull().any().any():
                logger.warning("Missing values detected in input data, filling with column means")
                self.data.fillna(self.data.mean(), inplace=True)
            
            # Basic range validation
            if (self.data['avg_humidity'] < 0).any() or (self.data['avg_humidity'] > 100).any():
                raise ValueError("Invalid humidity values: must be between 0 and 100")
            if (self.data['total_precipitation'] < 0).any() or (self.data['total_sunshine_hours'] < 0).any():
                raise ValueError("Negative precipitation or sunshine hours detected")
            if (self.data['ddd_demand'] < 0).any():
                raise ValueError("Negative demand values detected")
            
            # Drift detection
            for col, train_mean, train_std in zip(Config.EXPECTED_COLS, self.mean, self.std):
                recent_mean = self.data[col].mean()
                recent_std = self.data[col].std()
                mean_diff = abs(recent_mean - train_mean) / train_mean * 100 if train_mean != 0 else 0
                std_diff = abs(recent_std - train_std) / train_std * 100 if train_std != 0 else 0
                logger.info(f"{col} - Recent Mean: {recent_mean:.4f}, Std: {recent_std:.4f}")
                logger.info(f"{col} - Training Mean: {train_mean:.4f}, Std: {train_std:.4f}")
                logger.info(f"{col} - Drift: Mean diff {mean_diff:.2f}%, Std diff {std_diff:.2f}%")
                if mean_diff > 20 or std_diff > 20:
                    logger.warning(f"Potential data drift detected in {col}! Consider retraining.")
            
            logger.info("Loaded and validated last 12 months of input data (Apr 2024 - Mar 2025)")
        except Exception as e:
            logger.error(f"Failed to load data: {str(e)}")
            self.data = None
    
    def _load_predictions(self) -> None:
        """Load predictions from CSV file"""
        try:
            predictions_path = os.path.join(Config.DATA_DIR, 'predictions.csv')
            if not os.path.exists(predictions_path):
                raise FileNotFoundError(f"Predictions file {predictions_path} not found")
            predictions_df = pd.read_csv(predictions_path, parse_dates=['date'])
            required_cols = ['model_name', 'date', 'predicted_demand']
            if not all(col in predictions_df.columns for col in required_cols):
                raise ValueError(f"Missing required columns in predictions.csv: {required_cols}")
            self.predictions = {}
            for model in Config.MODEL_FILES.keys():
                model_preds = predictions_df[predictions_df['model_name'] == model]
                if model_preds.empty:
                    raise ValueError(f"No predictions found for model {model}")
                self.predictions[model] = {
                    pd.to_datetime(date).strftime('%B %Y'): float(row['predicted_demand'])
                    for date, row in model_preds[['date', 'predicted_demand']].iterrows()
                }
            logger.info("Loaded predictions")
        except Exception as e:
            logger.error(f"Failed to load predictions: {str(e)}")
            self.predictions = None
    
    def generate_predictions(self, input_data: pd.DataFrame, target_dates: List[str]) -> None:
        """Generate predictions for specified target dates using provided input data"""
        if not self.models or self.mean is None or self.std is None:
            raise HTTPException(status_code=500, detail="Models or normalization params not loaded")
        
        try:
            # Validate input data
            if input_data.shape[0] != 12 or input_data.shape[1] != len(Config.EXPECTED_COLS):
                raise ValueError("Input data must have 12 rows and 6 columns")
            if not all(col in input_data.columns for col in Config.EXPECTED_COLS):
                raise ValueError(f"Input data must have columns: {Config.EXPECTED_COLS}")
            
            # Normalize input data
            input_raw = input_data[Config.EXPECTED_COLS].values.astype('float32')
            input_normalized = (input_raw - self.mean) / self.std
            input_features = input_normalized[:, :-1]  # Exclude ddd_demand
            input_keras = input_features.reshape(1, 12, 5)
            
            # Generate predictions
            predictions_df = []
            ddd_mean = self.mean[-1]
            ddd_std = self.std[-1]
            for name, model in self.models.items():
                try:
                    y_pred = model.predict(input_keras, verbose=0).flatten()
                    y_pred_orig = y_pred * ddd_std + ddd_mean
                    for i, date in enumerate(target_dates):
                        predictions_df.append({
                            'model_name': name,
                            'date': date,
                            'predicted_demand': float(y_pred_orig[i])
                        })
                except Exception as e:
                    logger.error(f"Prediction error with {name}: {str(e)}")
            
            # Save predictions
            pd.DataFrame(predictions_df).to_csv(
                os.path.join(Config.DATA_DIR, 'predictions.csv'),
                index=False
            )
            logger.info(f"Saved predictions for {target_dates} to predictions.csv")
            
            # Drift check for input data
            for col, train_mean, train_std in zip(Config.EXPECTED_COLS, self.mean, self.std):
                recent_mean = input_data[col].mean()
                recent_std = input_data[col].std()
                mean_diff = abs(recent_mean - train_mean) / train_mean * 100 if train_mean != 0 else 0
                std_diff = abs(recent_std - train_std) / train_std * 100 if train_std != 0 else 0
                if mean_diff > 20 or std_diff > 20:
                    logger.warning(f"Potential data drift in {col} for new input: Mean diff {mean_diff:.2f}%, Std diff {std_diff:.2f}%")
        
        except Exception as e:
            logger.error(f"Failed to generate predictions: {str(e)}")
            raise HTTPException(status_code=500, detail="Prediction generation failed")
    
    def get_input_data(self) -> Dict:
        """Get the last 12 months of input data"""
        if self.data is None:
            logger.error("Input data not loaded")
            raise HTTPException(status_code=500, detail="Data not loaded")
        
        return {
            'data': self.data.reset_index().rename(columns={'index': 'date'}).to_dict(orient='records'),
            'stats': {
                'max_temp_avg': float(self.data['avg_temp_max'].mean()),
                'min_temp_avg': float(self.data['avg_temp_min'].mean()),
                'humidity_avg': float(self.data['avg_humidity'].mean()),
                'precip_total': float(self.data['total_precipitation'].sum()),
                'sunshine_total': float(self.data['total_sunshine_hours'].sum()),
                'current_demand': float(self.data['ddd_demand'].iloc[-1])
            }
        }
    
    def get_predictions(self) -> Dict:
        """Get predictions from loaded data"""
        if not self.predictions or self.test_maes is None or self.data is None:
            logger.error("Required data not loaded for predictions")
            raise HTTPException(status_code=500, detail="Predictions or data not loaded")
        
        return {
            'predictions': self.predictions,
            'best_model': min(self.test_maes, key=self.test_maes.get) if self.test_maes else None,
            'test_maes': self.test_maes,
            'last_6_months': self.data['ddd_demand'].iloc[-6:].reset_index().rename(columns={'index': 'date'}).to_dict(orient='records')
        }

# Initialize FastAPI app
app = FastAPI(
    title="Electricity Demand Forecasting Dashboard",
    description="OECD-style dashboard for electricity demand forecasting"
)

# CORS configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize model manager
model_manager = ModelManager()

@app.on_event("startup")
async def startup_event():
    model_manager.initialize()

# API endpoints
@app.get("/api/input-data", summary="Get input data used for forecasting")
async def get_input_data():
    return model_manager.get_input_data()

@app.get("/api/predictions", summary="Get model predictions")
async def get_predictions():
    return model_manager.get_predictions()

@app.post("/api/generate-predictions", summary="Generate predictions for specified dates")
async def generate_predictions(dates: List[str]):
    try:
        # Validate and convert dates
        target_dates = [pd.to_datetime(date).strftime('%Y-%m-%d') for date in dates]
        # Load input data (for simplicity, use latest available; in practice, provide new data)
        data_path = os.path.join(Config.DATA_DIR, 'selected_data.csv')
        df = pd.read_csv(data_path, header=0)
        start_date = pd.to_datetime('2017-01-01')
        dates_index = pd.date_range(start=start_date, periods=len(df), freq='MS')
        df.index = dates_index
        input_data = df.iloc[-12:].copy()  # Last 12 months; update for future months
        model_manager.generate_predictions(input_data, target_dates)
        return {"status": "success", "message": f"Predictions generated for {target_dates}"}
    except Exception as e:
        logger.error(f"API prediction error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# Mount static files
if os.path.exists(Config.STATIC_DIR):
    app.mount("/", StaticFiles(directory=Config.STATIC_DIR, html=True), name="static")
else:
    logger.warning(f"Static directory {Config.STATIC_DIR} not found")